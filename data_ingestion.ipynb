{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f20eff-13e9-454a-be32-0b4215c41eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasets langchain-community langchain-text-splitters faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd39faa-4c8c-480a-8dc3-a9e47b023257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9af8782-591b-4e90-bff3-4da3d4740541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"ChicagoHAI/CaseSumm\")\n",
    "\n",
    "# Convert the 'train' split to Pandas\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Select required columns and rename\n",
    "df = df[['opinion', 'syllabus']].rename(columns={\n",
    "    'opinion': 'text',\n",
    "    'syllabus': 'summary'\n",
    "})\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"legal.bronze.casesumm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e4d3ac-eb6b-4706-a175-61e15e53261d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from legal.bronze.casesumm limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a386a7-c488-4afb-8bff-4458452270ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "TABLE_NAME = \"legal.bronze.casesumm\"\n",
    "BATCH_SIZE = 5000  # number of rows per batch (tune for serverless memory)\n",
    "FAISS_DIR = \"/Volumes/legal/bronze/casesumm_volume/casesumm_faiss\"\n",
    "\n",
    "# --- Setup ---\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "embeddings = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "# Prepare an empty FAISS index for merging\n",
    "main_vectorstore = None\n",
    "\n",
    "# --- Process in batches ---\n",
    "batch_num = 0\n",
    "for batch_df in (\n",
    "    spark.table(TABLE_NAME)\n",
    "    .select(\"text\")\n",
    "    .where(\"text IS NOT NULL\")\n",
    "    .limit(1000000)  # optional: safety limit during first run\n",
    "    .toLocalIterator()  # stream to driver row-by-row\n",
    "):\n",
    "    batch_texts = []\n",
    "    for row in batch_df:\n",
    "        batch_texts.append(row)\n",
    "        if len(batch_texts) >= BATCH_SIZE:\n",
    "            # Process this batch\n",
    "            docs = splitter.create_documents(batch_texts)\n",
    "            vs = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "            if main_vectorstore is None:\n",
    "                main_vectorstore = vs\n",
    "            else:\n",
    "                main_vectorstore.merge_from(vs)\n",
    "\n",
    "            batch_texts = []\n",
    "            batch_num += 1\n",
    "            print(f\"Processed batch {batch_num}\")\n",
    "\n",
    "# Process any leftover texts\n",
    "if batch_texts:\n",
    "    docs = splitter.create_documents(batch_texts)\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    if main_vectorstore is None:\n",
    "        main_vectorstore = vs\n",
    "    else:\n",
    "        main_vectorstore.merge_from(vs)\n",
    "\n",
    "# --- Save final FAISS index ---\n",
    "os.makedirs(FAISS_DIR, exist_ok=True)\n",
    "main_vectorstore.save_local(FAISS_DIR)\n",
    "print(f\"âœ… FAISS index saved to {FAISS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e50ef0-b301-4231-8b3f-43fe6b4fc5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# config - change these to your values\n",
    "FAISS_DIR = \"/Volumes/legal/bronze/casesumm_volume/casesumm_faiss\"\n",
    "\n",
    "# create embeddings wrapper (used for loading the FAISS index)\n",
    "embeddings = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "# load the saved FAISS index\n",
    "vectorstore = FAISS.load_local(FAISS_DIR, embeddings)\n",
    "print(\"Loaded FAISS with\", vectorstore.index.ntotal, \"vectors (if available).\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8521091128145776,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
